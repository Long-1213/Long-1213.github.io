const generatedBibEntries = {
    "Beck2016_DocumentClassification": {
        "abstract": "To perform document classification algorithmically, documents need to be represented such that it is understandable to the machine learning classifier. The report discusses the different types of feature vectors through which document can be represented and later classified. The project aims at comparing the Binary, Count and TfIdf feature vectors and their impact on document classification. To test how well each of the three mentioned feature vectors perform, we used the 20-newsgroup dataset and converted the documents to all the three feature vectors. For each feature vector representation, we trained the Naïve Bayes classifier and then tested the generated classifier on test documents. In our results, we found that TfIdf performed 4% better than Count vectorizer and 6% better than Binary vectorizer if stop words are removed. If stop words are not removed, then TfIdf performed 6% better than Binary vectorizer and 11% better than Count vectorizer. Also, Count vectorizer performs better than Binary vectorizer, if stop words are removed by 2% but lags behind by 5% if stop words are not removed. Thus, we can conclude that TfIdf should be the preferred vectorizer for document representation and classification.",
        "author": "Leonard Wesley ;Robert Chun ;Robin James",
        "doi": "https://doi.org/10.31979/etd.6jmu-9xdt",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:system, visual_analytics, sparklines, information_retrieval, clustering, literature_browser",
        "number": "01",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "DOCUMENT CLASSIFICATION USING MACHINE LEARNING",
        "type": "article",
        "url": "https://scholarworks.sjsu.edu/etd_projects/531/",
        "volume": "22",
        "year": "2016"
    },
    "Liu2006_AutomaticTableMetadata": {
        "abstract": "Tables are used to present, list, summarize, and structure important data in documents. In scholarly articles, they are often used to present the relationships among data and high-light a collection of results obtained from experiments and scientific analysis. In digital libraries, extracting this data automatically and understanding the structure and content of tables are very important to many applications. Automatic identification extraction, and search for the contents of tables can be made more precise with the help of metadata. In this paper, we propose a set of medium-independent table metadata to facilitate the table indexing, searching, and exchanging. To extract the contents of tables and their metadata, an automatic table metadata extraction algorithm is designed and tested on PDF documents.",
        "author": "Ying Liu, Prasenjit Mitra, C. Lee Giles, Kun Bai",
        "doi": "https://doi.org/10.1145/1141753.1141835",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:article, visual_analytics, sparklines, information_retrieval, clustering, literature_browser",
        "number": "01",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Automatic extraction of table metadata from digital documents",
        "type": "article",
        "url": "https://dl.acm.org/doi/abs/10.1145/1141753.1141835",
        "volume": "22",
        "year": "2006"
    },
    "Hong2021_InformationExtractionReview": {
        "abstract": "Tables are used to present, list, summarize, and structure important data in documents. In scholarly articles, they are often used to present the relationships among data and high-light a collection of results obtained from experiments and scientific analysis. In digital libraries, extracting this data automatically and understanding the structure and content of tables are very important to many applications. Automatic identification extraction, and search for the contents of tables can be made more precise with the help of metadata. In this paper, we propose a set of medium-independent table metadata to facilitate the table indexing, searching, and exchanging. To extract the contents of tables and their metadata, an automatic table metadata extraction algorithm is designed and tested on PDF documents.",
        "author": "YZhi Hong, Logan Ward, Kyle Chard, Ben Blaiszik & Ian Foster ",
        "doi": "https://doi.org/10.1007/s11837-021-04902-9",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:article, visual_analytics, sparklines, information_retrieval, clustering, literature_browser",
        "number": "01",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Challenges and Advances in Information Extraction from Scientific Literature: a Review",
        "type": "article",
        "url": "https://link.springer.com/article/10.1007/s11837-021-04902-9",
        "volume": "22",
        "year": "2021"
    },
    "Ying2018_BigDataManufacturing": {
        "abstract": "Driven by the innovative improvement of information and communication technologies (ICTs) and their applications into manufacturing industry, the big data era in manufacturing is correspondingly arising, and the developing data mining techniques (DMTs) pave the way for pursuing the aims of smart production with the real-time, dynamic, self-adaptive and precise control. However, lots of factors in the ever-changing environment of manufacturing industry, such as, various of complex production processes, larger scale and uncertainties, more complicated constrains, coupling of operational performance, and so on, make production management face with more and more big challenges. The dynamic inflow of a large number of raw data which is collected from the physical manufacturing sites or generated in various related information systems, caused the heavy information overload problems. Indeed, most of traditional DMTs are not yet sufficient to process such big data for smart production management. Therefore, this paper reviews the development of DMTs in the big data era, and makes discussion on the applications of DMTs in production management, by selecting and analyzing the relevant papers since 2010. In the meantime, we point out limitations and put forward some suggestions about the smartness and further applications of DMTs used in production management.",
        "author": "Cheng Ying, Chen Ken, Sun Hemeng, Zhang Yongping, Tao Fei",
        "doi": "https://doi.org/10.1016/j.jii.2017.08.001",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:article, visual_analytics, sparklines, information_retrieval, clustering, literature_browser",
        "number": "01",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Data and knowledge mining with big data towards smart production",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/abs/pii/S2452414X17300584",
        "volume": "22",
        "year": "2018"
    },
    "Salau2019_FeatureExtractionSurvey": {
        "abstract": "Feature extraction (FE) is an important step in image retrieval, image processing, data mining and computer vision. FE is the process of extracting relevant information from raw data. However, the problem of extracting appropriate features that can reflect the intrinsic content of a piece of data or dataset as complete as possible is still a challenge for most FE techniques. In this paper, we present a survey of the existing FE techniques used in recent times. In this study, it was observed that the most unique features that can be extracted when using GLDS features on images are contrast, homogeneity, entropy, mean and energy. In addition, it was observed that FE techniques are not mainly application specific but can be applied to several applications.",
        "author": "Ayodeji Olalekan Salau; Shruti Jain",
        "doi": "10.1109/ICSC45622.2019.8938371",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:article, visual_analytics, sparklines, information_retrieval, clustering, literature_browser",
        "number": "01",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Feature Extraction: A Survey of the Types, Techniques, Applications",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/document/8938371",
        "volume": "22",
        "year": "2019"
    },
    "Ramakrishnan2012_LayoutAwareTextExtraction": {
        "abstract": "The Portable Document Format (PDF) is the most commonly used file format for online scientific publications. The absence of effective means to extract text from these PDF files in a layout-aware manner presents a significant challenge for developers of biomedical text mining or biocuration informatics systems that use published literature as an information source. In this paper we introduce the ‘Layout-Aware PDF Text Extraction’ (LA-PDFText) system to facilitate accurate extraction of text from PDF files of research articles for use in text mining applications.",
        "author": "Cartic Ramakrishnan, Abhishek Patnia, Eduard Hovy & Gully APC Burns",
        "doi": "https://doi.org/10.1186/1751-0473-7-7",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:article, visual_analytics, sparklines, information_retrieval, clustering, literature_browser",
        "number": "01",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Layout-aware text extraction from full-text PDF of scientific articles",
        "type": "article",
        "url": "https://link.springer.com/article/10.1186/1751-0473-7-7",
        "volume": "22",
        "year": "2012"
    },
    "Ingemarsson2024_PDFParsing": {
        "abstract": "This report investigates the challenges and methods associated with parsing PDF documents for use with Large Language Models (LLMs). The variability and complexity of PDF formats pose significant challenges in ensuring accurate data extraction and interpretation. We evaluate several parsing techniques, including rule-based, deep learning-based, and multimodal methods, to determine their effectiveness in handling diverse PDF content. Our study reveals that while multimodal methods generally outperform others, particularly in managing mixed content formats. For PDFs containging complex images a deep learning approach is more effective. This research offers valuable insights into optimizing PDF parsing strategies, balancing accuracy and cost-efficiency, thereby advancing the utility of LLMs in document processing and contributing to improved data management practices in various industries.",
        "author": "Ingemarsson, PerDaniel, Persson",
        "doi": "https://doi.org/10.1186/1751-0473-7-7",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:article, visual_analytics, sparklines, information_retrieval, clustering, literature_browser",
        "number": "01",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "PDF Parsing, Unveiling the Most Efficient Method",
        "type": "article",
        "url": "https://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1920553&dswid=8811",
        "volume": "22",
        "year": "2024"
    },
    "Sheffer2017_TabularContentExtraction": {
        "abstract": "Portable Document Format (PDF) has been a popular way to exchange data in documents since Adobe introduced the format in 1993. Its report-like characteristic which preserves and prioritizes graphical visualization was part of the main publishing concerns among several segments including government agencies. In this way, tabular data started to be enclosed within PDF documents and disclosed in government portals. This situation, apart being surprisingly contradictory to data openness, is still found even in the major open data initiatives. It is estimated that roughly 13% of published files in some main open data portals around the world have their data made available in PDF. Thus, there is a need for effective tools capable of extracting tabular content (a main placeholder for data) from PDF to allow its data to be published in more open formats such as the well-known CSV which complies with accessible and machine processable open data principles.This paper aims at providing a structured and comprehensive overview of the research in tabular content extraction specifically from PDF documents as well as to provide an overview of most recent practical results in the literature. The contribution of this work goes beyond theoretical discussions by helping data practitioners to understand to what extent methods and tools regarding tabular content extraction from PDF can benefit the open data initiatives in practical and effective ways.",
        "author": "Andreiwid Sheffer Corrêa, Pär-Ola Zander",
        "doi": "https://doi.org/10.1145/3085228.308527",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:article, visual_analytics, sparklines, information_retrieval, clustering, literature_browser",
        "number": "01",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Unleashing Tabular Content to Open Data: A Survey on PDF Table Extraction Methods and Tools",
        "type": "article",
        "url": "https://dl.acm.org/doi/abs/10.1145/3085228.3085278",
        "volume": "22",
        "year": "2017"
    },
    "Salloum2017_TextMiningResearch": {
        "abstract": "Nowadays, research in text mining has become one of the widespread fields in analyzing natural language documents. The present study demonstrates a comprehensive overview about text mining and its current research status. As indicated in the literature, there is a limitation in addressing Information Extraction from research articles using Data Mining techniques. The synergy between them helps to discover different interesting text patterns in the retrieved articles. In our study, we collected, and textually analyzed through various text mining techniques, three hundred refereed journal articles in the field of mobile learning from six scientific databases, namely: Springer, Wiley, Science Direct, SAGE, IEEE, and Cambridge. The selection of the collected articles was based on the criteria that all these articles should incorporate mobile learning as the main component in the higher educational context. Experimental results indicated that Springer database represents the main source for research articles in the field of mobile education for the medical domain. Moreover, results where the similarity among topics could not be detected were due to either their interrelations or ambiguity in their meaning. Furthermore, findings showed that there was a booming increase in the number of published articles during the years 2015 through 2016. In addition, other implications and future perspectives are presented in the study.",
        "author": "Said A. Salloum, Mostafa Al-Emran, Azza Abdel Monem & Khaled Shaalan",
        "doi": "https://doi.org/10.1007/978-3-319-67056-0_18",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:article, visual_analytics, sparklines, information_retrieval, clustering, literature_browser",
        "number": "01",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Using Text Mining Techniques for Extracting Information from Research Articles",
        "type": "article",
        "url": "https://link.springer.com/chapter/10.1007/978-3-319-67056-0_18",
        "volume": "22",
        "year": "2017"
    },
    "Safder2020_DocumentAnalysis": {
        "abstract": "The advancements of search engines for traditional text documents have enabled the effective retrieval of massive textual information in a resource-efficient manner. However, such conventional search methodologies often suffer from poor retrieval accuracy especially when documents exhibit unique properties that behoove specialized and deeper semantic extraction. Recently, AlgorithmSeer, a search engine for algorithms has been proposed, that extracts pseudo-codes and shallow textual metadata from scientific publications and treats them as traditional documents so that the conventional search engine methodology could be applied. However, such a system fails to facilitate user search queries that seek to identify algorithm-specific information, such as the datasets on which algorithms operate, the performance of algorithms, and runtime complexity, etc. In this paper, a set of enhancements to the previously proposed algorithm search engine are presented. Specifically, we propose a set of methods to automatically identify and extract algorithmic pseudo-codes and the sentences that convey related algorithmic metadata using a set of machine-learning techniques. In an experiment with over 93,000 text lines, we introduce 60 novel features, comprising content-based, font style based and structure-based feature groups, to extract algorithmic pseudo-codes. Our proposed pseudo-code extraction method achieves 93.32% F1-score, outperforming the state-of-the-art techniques by 28%. Additionally, we propose a method to extract algorithmic-related sentences using deep neural networks and achieve an accuracy of 78.5%, outperforming a Rule-based model and a support vector machine model by 28% and 16%, respectively.",
        "author": "Iqra Safder , Saeed-Ul Hassan , Anna Visvizi , Thanapon Noraset , Raheel Nawaz , Suppawong Tuarob ",
        "doi": "https://doi.org/10.1016/j.ipm.2020.102269",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:article, visual_analytics, sparklines, information_retrieval, clustering, literature_browser",
        "number": "01",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Document analysis of PDF files: methods, results and implications",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/abs/pii/S0306457319312610",
        "volume": "22",
        "year": "2020"
    }
};
